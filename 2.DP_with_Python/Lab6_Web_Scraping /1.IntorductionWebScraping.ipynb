{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4103946724.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    url = http://... # url of web page\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "url = http://... # url of web page\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "html_page = requests.get(URL, headers={\"User-Agent\":\"Mozilla/5.0\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>Profile: Aphrodite</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/aphrodite.gif\" />\n",
      "<h2>Name: Aphrodite</h2>\n",
      "<br><br>\n",
      "Favorite animal: Dove\n",
      "<br><br>\n",
      "Favorite color: Red\n",
      "<br><br>\n",
      "Hometown: Mount Olympus\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Profile: Aphrodite'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "page = urlopen(url)\n",
    "page\n",
    "\n",
    "#extract html from the page. 1. use the httpsResponse objects .read() (return a sequence of bytes); 2. decodde( to decode the bytes)\n",
    "html_bytes = page.read()\n",
    "html = html_bytes.decode(\"utf-8\")\n",
    "print(html)\n",
    "\n",
    "#extract text from html - String methods\n",
    "#.find() \n",
    "title_index = html.find(\"<title>\") #the index of the title tag 14\n",
    "title_index\n",
    "\n",
    "#index of the title itself\n",
    "start_index = title_index + len(\"<title>\")\n",
    "start_index #21\n",
    "\n",
    "#index of the closing </title> tag\n",
    "end_index = html.find(\"</title>\")\n",
    "end_index #39\n",
    "\n",
    "#slicing the html string\n",
    "title = html[start_index:end_index]\n",
    "title   #'Profile: Aphrodite'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<head>\\n<title >Profile: Poseidon'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another example, problem\n",
    "url = \"http://olympus.realpython.org/profiles/poseidon\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "start_index = html.find(\"<title>\") + len(\"<title>\")\n",
    "end_index = html.find(\"</title>\")\n",
    "title = html[start_index:end_index]\n",
    "title #'\\n<head>\\n<title >Profile: Poseidon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac']\n",
      "['abc', 'ac']\n",
      "['ABC']\n",
      "['abc']\n",
      "[]\n",
      "['sanda']\n",
      "Everything is Elephants\n",
      "Everything is Elephants if it's in Elephants\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.findall(\"ab*c\", \"ac\"))\n",
    "print(re.findall(\"ab*c\", \"abcac\"))\n",
    "print(re.findall(\"ab*c\", \"ABC\", re.IGNORECASE))\n",
    "\n",
    "print(re.findall(\"a.c\", \"abc\")) #find all the strings that contain the letters \"a\" and \"c\"\n",
    "print(re.findall(\"a.c\", \"abbvvc\"))\n",
    "\n",
    "print(re.findall(\"s.*a\", \"sanda\"))\n",
    "\n",
    "match_results = re.search(\"ab*c\", \"ABC\", re.IGNORECASE)\n",
    "#match_results.group()\n",
    "\n",
    "#e.sub() uses the regular expression \"<.*>\" to find and replace everything between the first \n",
    "# < and the last >, which spans from the beginning of <replaced> to the end of <tags>.\n",
    "#Python’s regular expressions are greedy, meaning they try to find the longest possible match when characters like * are used.\n",
    "string = \"Everything is <replaced> if it's in <tags>\"\n",
    "string = re.sub(\"<.*>\", \"Elephants\", string)\n",
    "print(string)\n",
    "\n",
    "string = \"Everything is <replaced> if it's in <tags>\"\n",
    "string = re.sub(\"<.*?>\", \"Elephants\", string)\n",
    "print(string)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text From HTML With Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: Dionysus\n"
     ]
    }
   ],
   "source": [
    "#parse out the title from abother website\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "\n",
    "pattern = \"<title.*?>.*?</title.*?>\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "title = match_results.group()\n",
    "title = re.sub(\"<.*?>\", \"\", title) #remove html tags\n",
    "print(title)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dionysus\n",
      "Wine\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "html_page = urlopen(url)\n",
    "html_text = html_page.read().decode(\"utf-8\")\n",
    "\n",
    "#pattern = \"<h2.*?>.*?</h2.*?>\"\n",
    "#match_res = re.search(pattern, html, re.IGNORECASE)\n",
    "#title = match_res.group()\n",
    "#title = re.sub(\"<.*?>\", \"\", title)\n",
    "#print(title)\n",
    "\n",
    "for string in [\"Name: \", \"Favorite Color:\"]:\n",
    "    string_start_idx = html_text.find(string)\n",
    "    text_start_idx = string_start_idx + len(string)\n",
    "\n",
    "    next_html_tag_offset = html_text[text_start_idx:].find(\"<\")\n",
    "    text_end_idx = text_start_idx + next_html_tag_offset\n",
    "\n",
    "    raw_text = html_text[text_start_idx : text_end_idx]\n",
    "    clean_text = raw_text.strip(\" \\r\\n\\t\")\n",
    "    print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use html_text.find() to find the starting index of the string, either \"Name:\" or \"Favorite Color:\", and then assign the index to string_start_idx.\n",
    "\n",
    "Since the text to extract starts just after the colon in \"Name:\" or \"Favorite Color:\", you get the index of the character immediately after the colon by adding the length of the string to start_string_idx, and then assign the result to text_start_idx.\n",
    "\n",
    "You calculate the ending index of the text to extract by determining the index of the first angle bracket (<) relative to text_start_idx and assign this value to next_html_tag_offset. Then you add that value to text_start_idx and assign the result to text_end_idx.\n",
    "\n",
    "You extract the text by slicing html_text from text_start_idx to text_end_idx and assign this string to raw_text.\n",
    "\n",
    "You remove any whitespace from the beginning and end of raw_text using .strip() and assign the result to clean_text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an HTML parser for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profile: Dionysus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name: Dionysus\n",
      "\n",
      "Hometown: Mount Olympus\n",
      "\n",
      "Favorite animal: Leopard \n",
      "\n",
      "Favorite Color: Wine\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/static/dionysus.jpg\n",
      "/static/grapes.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<img src=\"/static/dionysus.jpg\"/>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(soup.get_text())\n",
    "\n",
    "soup.find_all(\"img\")\n",
    "image1, image2 = soup.find_all(\"img\")\n",
    "image1.name\n",
    "\n",
    "print(image1[\"src\"])\n",
    "print(image2[\"src\"])\n",
    "\n",
    "soup.title.string\n",
    "soup.find_all(\"img\", src=\"/static/dionysus.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opens the URL http://olympus.realpython.org/profiles/dionysus by using urlopen() from the urllib.request module\n",
    "\n",
    "Reads the HTML from the page as a string and assigns it to the html variable\n",
    "\n",
    "Creates a BeautifulSoup object and assigns it to the soup variable\n",
    "\n",
    "The BeautifulSoup object assigned to soup is created with two arguments. The first argument is the HTML to be parsed, and the second argument, the string \"html.parser\", tells the object which parser to use behind the scenes. \"html.parser\" represents Python’s built-in HTML parser.\n",
    "\n",
    "soup.get_text() : There are a lot of blank lines in this output. These are the result of newline characters in the HTML document’s text. You can remove them with the .replace() string method if you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program that grabs the full HTML from the page at the URL http://olympus.realpython.org/profiles.\n",
    "\n",
    "Using Beautiful Soup, print out a list of all the links on the page by looking for HTML tags with the name a and retrieving the value taken on by the href attribute of each tag.\n",
    "\n",
    "output:\n",
    "http://olympus.realpython.org/profiles/aphrodite\n",
    "http://olympus.realpython.org/profiles/poseidon\n",
    "http://olympus.realpython.org/profiles/dionysus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://olympus.realpython.org/profiles/aphrodite\n",
      "http://olympus.realpython.org/profiles/poseidon\n",
      "http://olympus.realpython.org/profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"http://olympus.realpython.org\"\n",
    "\n",
    "html_page = urlopen(base_url + \"/profiles\")\n",
    "html_text = html_page.read().decode(\"utf-8\")\n",
    "\n",
    "soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "#soup.find_all(\"a\") returns a list of all the links in the HTML source\n",
    "for link in soup.find_all(\"a\"):\n",
    "    link_url = base_url + link[\"href\"]\n",
    "    print(link_url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact With HTML Forms\n",
    "The Python standard library doesn’t provide a built-in means for working with web pages interactively, but many third-party packages are available from PyPI. Among these, MechanicalSoup is a popular and relatively straightforward package to use.\n",
    "\n",
    "In essence, MechanicalSoup installs what’s known as a headless browser, which is a web browser with no graphical user interface. This browser is controlled programmatically via a Python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "<html>\n",
      "<head>\n",
      "<title>Log In</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br/><br/>\n",
      "<h2>Please log in to access Mount Olympus:</h2>\n",
      "<br/><br/>\n",
      "<form action=\"/login\" method=\"post\" name=\"login\">\n",
      "Username: <input name=\"user\" type=\"text\"/><br/>\n",
      "Password: <input name=\"pwd\" type=\"password\"/><br/><br/>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form>\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "browser = mechanicalsoup.Browser()\n",
    "\n",
    "url = \"http://olympus.realpython.org/login\"\n",
    "page = browser.get(url)\n",
    "print(page)\n",
    "\n",
    "print(type(page.soup))\n",
    "\n",
    "print(page.soup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "status code 200 means that the request was successful\n",
    "\n",
    "MechanicalSoup uses Beautiful Soup to parse the HTML from the request, and page has a .soup attribute that represents a BeautifulSoup object\n",
    "\n",
    "You can view the HTML by inspecting the .soup attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a Form With MechanicalSoup\n",
    "1 You create a Browser instance and use it to request the URL http://olympus.realpython.org/login. You assign the HTML content of the page to the login_html variable using the .soup property.\n",
    "\n",
    "2 login_html.select(\"form\") returns a list of all <form> elements on the page. Because the page has only one <form> element, you can access the form by retrieving the element at index 0 of the list. When there is only one form on a page, you may also use login_html.form. The next two lines select the username and password inputs and set their value to \"zeus\" and \"ThunderDude\", respectively.\n",
    "\n",
    "3  You submit the form with browser.submit(). Notice that you pass two arguments to this method, the form object and the URL of the login_page, which you access via login_page.url.\n",
    "\n",
    "In the interactive window, you confirm that the submission successfully redirected to the /profiles page. If something had gone wrong, then the value of profiles_page.url would still be \"http://olympus.realpython.org/login\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://olympus.realpython.org/profiles\n",
      "Aphrodite: /profiles/aphrodite\n",
      "Poseidon: /profiles/poseidon\n",
      "Dionysus: /profiles/dionysus\n",
      "Aphrodite: /profiles/aphrodite\n",
      "Poseidon: /profiles/poseidon\n",
      "Dionysus: /profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "\n",
    "#1\n",
    "browser = mechanicalsoup.Browser()\n",
    "url =  \"http://olympus.realpython.org/login\"\n",
    "login_page = browser.get(url)\n",
    "login_html = login_page.soup\n",
    "\n",
    "#2\n",
    "form = login_html.select(\"form\")[0]\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "\n",
    "#3\n",
    "profiles_page = browser.submit(form, login_page.url)\n",
    "\n",
    "print(profiles_page.url)\n",
    "\n",
    "links = profiles_page.soup.select(\"a\")\n",
    "for link in links:\n",
    "    address = link[\"href\"]\n",
    "    text = link.text\n",
    "    print(f\"{text}: {address}\")\n",
    "\n",
    "base_url = \"http://olympus.realpython.org\"\n",
    "for link in links:\n",
    "    address = link[\"href\"]\n",
    "    text = link.text\n",
    "    print(f\"{text}: {address}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact With Websites in Real Time\n",
    "Sometimes you want to be able to fetch real-time data from a website that offers continually updated information.\n",
    "\n",
    "This example uses the BeautifulSoup object’s .select() method to find the element with id=result. The string \"#result\", which you pass to .select(), uses the CSS ID selector # to indicate that result is an id value.\n",
    "\n",
    "To periodically get a new result, you’ll need to create a loop that loads the page at each step. So everything below the line browser = mechanicalsoup.Browser() in the above code needs to go in the body of the loop.\n",
    "\n",
    "For this example, you want four rolls of the dice at ten-second intervals. To do that, the last line of your code needs to tell Python to pause running for ten seconds. You can do this with .sleep() from Python’s time module. The .sleep() method takes a single argument that represents the amount of time to sleep in seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of your dice roll is: 6\n",
      "The result of your dice roll is: 3\n",
      "The result of your dice roll is: 5\n",
      "The result of your dice roll is: 1\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "import time\n",
    "\n",
    "browser = mechanicalsoup.Browser()\n",
    "\n",
    "for i in  range(4):\n",
    "    page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "    tag = page.soup.select(\"#result\")[0]\n",
    "    result = tag.text\n",
    "\n",
    "    print(f\"The result of your dice roll is: {result}\")\n",
    "    if i < 3:\n",
    "        time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm about to wait for 5 seconds ...\n",
      "Done waiting!\n"
     ]
    }
   ],
   "source": [
    "#seep method\n",
    "import time\n",
    "print(\"I'm about to wait for 5 seconds ...\")\n",
    "time.sleep(5)\n",
    "print(\"Done waiting!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
